{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNL/UDhFj5+yiSUgNkeS4G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/combinatoricsFormula/Git-Github-Automation/blob/main/satistics_r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction:**\n",
        "---\n",
        "Before we start our journey, let me set the expectation for our course. In the real world world, we rarely have unifrmal and cleaned datasets. We have to capture our own dataset, obtain it from external sources, or even sometimes similate through randomization. In every case, we are required to standerize the data. Please note, even though the course is statistics with R, We are going to use other programming languages that best fit based on the task we require. You might see SQL, Python, Javascript, PHP, CSS, HTML, C, and Assembly Language  depending on the feature requirement. We are doing it this way because real Data Engineers have to model and source data from different platforms. I know the order of how I approached things is different from tradition because often times companies spend millions of dollars each year on data munipilation rather than optimizing the insight analysis, therefore, you might see the same steps again in each chapter.\n",
        "---\n",
        "\n",
        "**Chapter one: **\n",
        "*   Data processing & Capturing Stages\n",
        "  * Data Collection and Ingestion\n",
        "      * Building our minimilistic input form\n",
        "      * Building API\n",
        "      * Scraping\n",
        "      * IoT Devices\n",
        "      * Building our Mysql Database\n",
        "      * Exporting Datasets from Database\n",
        "      * Similating internal / External sources\n",
        "      * Our own random Dataset\n",
        "      * Sensor Data from a device\n",
        "      * Building Client data drop server\n",
        "      * Building tracker of dropped files\n",
        "  * Data Cleaning (Data Quality Assurance)\n",
        "      * Building our standerized data form\n",
        "      * Transforming all data to meet desired form\n",
        "      * Loading Datasets\n",
        "      * Investigating Datasets\n",
        "  * Data Integration and Merging\n",
        "      * Merging Datasets sources\n",
        "      * Cleaning Datasets\n",
        "      * Datasets standerizations\n",
        "  * Exploratory Data Analysis (EDA)\n",
        "      * Generate summary statistics (mean, median, variance).\n",
        "      * Visualize data through plots (histograms, scatter plots, boxplots).\n",
        "      * Identify correlations and anomalies.\n",
        "\n",
        "  * Data Wrangling (Transforming and Structuring Data)\n",
        "      * Running Discriptive Data Analysis\n",
        "      * Building reusable Discriptive tool\n",
        "  * Data Standardization\n",
        "      * Data modeling principles (relational and non-relational).\n",
        "      * Identification of Datatypes\n",
        "      * Identification of Applicable Distribution\n",
        "  * Data Normalization\n",
        "      * Apply techniques like Min-Max scaling, Z-score normalization, or log transformation.\n",
        "      * Normalize relational databases (1NF, 2NF, 3NF).\n",
        "  *   Data Validation\n",
        "      * Let us excel and pivot tables just to go back in time\n",
        "  * Automation of Data Preparation\n",
        "      * AUTOMATED DATA PIPELINES\n",
        "      * Automate ETL pipelines.\n",
        "      * Schedule data cleaning jobs.\n",
        "  * Ensuring Security and Privacy Compliance\n",
        "      * There is no benifits building something another can destroy!\n",
        "\n",
        "  * DOCUMENTATION AND BEST PRACTICES\n",
        "      * We get to learn Git\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y7wrNUFSLsq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Let us start with Building the web form to have our agents capture data from clients. We are similating a credit card company here.\n",
        "The reson we picked this industry is because it will tie into our inference analysis later.'''"
      ],
      "metadata": {
        "id": "QlRKVknJLfo7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}